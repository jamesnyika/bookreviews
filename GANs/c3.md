
# Chapter 3 Your First GAN: Generating Handwritten Digits
---

  One of the best chapters introducing the topic. I really enjoyed the way in which it describes each component of a GAN.

  ## Highlights
  * Clear summarization of what a GAN is composed of.
  * Good Table breaking apart the input, outputs etc of each. Very very useful and clarifying the structure

  ## Improvements
  * Page 9 : "Unlike regular ReLU function which maps any negative input
to 0, Leaky ReLU allows a small positive gradient. This prevents greadients from dying
out during training , which tends to yield better training outcomes.".
        * spelling mistake : greadients
        * Can you describe what the different outcomes would be. Kindly link gradients to outcome. As a reader, I have no idea, necessarily why I should pick a Leaky ReLU vs a regular ReLU. It is a technical point at a teachable moment :-)

        
