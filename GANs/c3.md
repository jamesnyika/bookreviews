
# Chapter 3 Your First GAN: Generating Handwritten Digits
---

  One of the best chapters introducing the topic. I really enjoyed the way in which it describes each component of a GAN.

  ## Highlights
  * Clear summarization of what a GAN is composed of.
  * Good Table breaking apart the input, outputs etc of each. Very very useful and clarifying the structure

  ## Improvements
  * Page 9 : "Unlike regular ReLU function which maps any negative input
to 0, Leaky ReLU allows a small positive gradient. This prevents greadients from dying
out during training , which tends to yield better training outcomes.".
        * spelling mistake : greadients
        * Can you describe what the different outcomes would be. Kindly link gradients to outcome. As a reader, I have no idea, necessarily why I should pick a Leaky ReLU vs a regular ReLU. It is a technical point at a teachable moment :-)

* Page 10 : "The reason for using tanh (as opposed to, say, sigmoid which would output values in the more typical 0 to 1 range) is that tanh tends to produce crisper images."  - I understand what the functions will do to the output but I am not linking this back to the real world output of why the images are crisper with a tanh scaling function. Would it be even crisper if i scaled even higher or not ? If I was trying to muddle a picture, would a function scaling between -0.5 and +0.5 work better. As the inexperienced reader I want to know why - what is the intuition ?

* Page 11 - use of Binary Cross Entropy as the loss function - Reasoning behind the choice ? Is there a close corollary ? Perhaps a suggestion for an alternative if your library has other choices.
