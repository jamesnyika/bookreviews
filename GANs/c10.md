
# Chapter 10 CycleGAN
---
  This proved to be quite a difficult chapter to understand.
  * The notation in the table particularly of the loss got me lost. I appreciate the footnote but I wish the was another way to outline or explain the losses.   
  * This note : "But for now, we will simply think of the two mappings as two autoencoders: F(G(a)) and G(F(b)). In other words, we take the basic idea of the autoencoder—including a kind of “explicit” loss function as substituted by the Cycle-Consistency loss—and add discriminators to it." .. really helped clarify a little more what we were dealing with.

  * Appreciated the explanation of how the CNN are used. These are generally familiar so it was helpful to see and understand what they were doing. I am not familiar with ResNet so I was not able to relate back your comment related to architecture

  ## Highlights
  * Summary table for comparison.
  * Diagrams showing the architecture.
