
# Chapter 7 Semi-Supervised GANS
---
The chart of the count of types of GAN papers was a very good way to illustrate the growth of the
general interest in GANs. It also did generate some dismay at the challenge of understanding how one can understanding
all that material.

Provided you cover the most prominent techniques in this book, then I think you do the best service to the community.

A question that you can perhaps address is the relationship of computing power to the successful training of GANs or NNs in general. If we could double or quadruple the compute available and parallelization of that compute - what would it do for GANs and Neural Networks ?

Another question - Could GANs be uses to generate a pre-labelled data sets for training ? they currently can generate very high fidelity images - so is labelling data out the realm of possibility ?



  ## Highlights
  * Table 7.1 is an excellent summary. It helps quickly reference the differences and I would include it in a cheat sheet if I were to build one.
  * With a lot more training data, the fully-supervised classifier’s ability to generalize
improves dramatically
  *

  ## Recommendations
  * Where you mention "In practice, the contrast is even starker than the one shown, with labeled data comprising only a tiny fraction (often as little as 1% or 2%) of the training data." - Can you give the intuition behind the amazingly small quantity of labelled instances ? It seems to be hard to understand relative to the statement : "...With a lot more training data, the fully-supervised classifier’s ability to generalize improves dramatically" - If indeed, with more training you get better results.. why bother with using small training amounts ? In short - what other value then is there to using an SGAN with little data if using lots will just do a much better job...

  * 
