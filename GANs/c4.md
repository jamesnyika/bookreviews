
# Chapter 4 Deep Convolutional GAN (DCGAN)
---
  ## Highlights
  * Working Notebook code
  * Recap of the CNNs relative to single layer connected networks. This is actually
    a difference I was not aware of until you highlighted it.
  * A superb chapter because it helps the reader understand that the same task can be performed through different configurations of a GAN.


  ## Suggestions for improvement

  * Please provide a way to quantify/intuitively understand the cost of using this approach to perform the task vs using the GAN in Chapter 3 other than just better results.
   
  * Please add in the book somewhere a comparison of the different types of GANs to compare them on some
   dimensions. For example. A table that differentiates them by the types of input they typically process; how deep the layers
    normally are; fully connected or not; do they reduce dimensionality or not; typical output type etc.

  * Page 6 - reference to the minibatch could be clearer. The concept of normalization is well explained but if you could talk a little bit about how the batch is selected it would clarify further.

  * Page 9 - I enjoyed the description of the transposed convolutions basically going all the way back.
      * Can you guide us by indicating how one should think about how many deconvolution layers to use ? If we took more intermediate steps - other than compute,.. what is the cost/benefit decision there?

      * How should we interpret the choice of the tanh function ? is there any appreciable difference to using a similar function to the tanh like an ArcTan or a PreLU ?

  * Page 17 - A question I had from the statement "As evident from the figures above, all the extra work we put into implementing DCGAN paid off handsomely." is ... why ? Does the batch normalization help reduce some of the noise that appears in the Chapter 3 GAN images ? Do you have some intuition to support the evidence as we see it ?
